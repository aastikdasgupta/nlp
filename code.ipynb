{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c95a6d",
   "metadata": {},
   "source": [
    "__IMPORTING PYTHON MODULES__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df2b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb27c88",
   "metadata": {},
   "source": [
    "__READING INPUT EXCEL SHEET__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be82fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.read_excel(\"input.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e5f4bf",
   "metadata": {},
   "source": [
    "__SCRAPING CODE (DATA ALREADY SCRAPED AND SAVED IN DIFFERENT FOLDER)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a4553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(input_data)):\n",
    "#     if(i in [7,20,107]):\n",
    "#         continue\n",
    "#     else:\n",
    "#         url = input_data['URL'][i]\n",
    "#         page = requests.get(url)\n",
    "#         soup = bs(page.content,'html.parser')\n",
    "#         heading = soup.find('h1', class_ = [\"entry-title\",\"tdb-title-text\"]).text\n",
    "#         para = soup.find_all('p', attrs={\"class\": None})\n",
    "#         with open(os.path.join('datatxt','{}.txt'.format(input_data['URL_ID'][i])), 'w', encoding = 'utf8') as f:\n",
    "#             f.write(heading)\n",
    "#             f.write('\\n')\n",
    "#         for p in para:\n",
    "#             with open(os.path.join('datatxt','{}.txt'.format(input_data['URL_ID'][i])), 'a', encoding = 'utf8') as f:\n",
    "#                 f.write(p.text)\n",
    "#                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d5061",
   "metadata": {},
   "source": [
    "__IMPORTING NLTK MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7142e673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading cmudict: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10060] A connection attempt failed because\n",
      "[nltk_data]     the connected party did not properly respond after a\n",
      "[nltk_data]     period of time, or established connection failed\n",
      "[nltk_data]     because connected host has failed to respond>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c0c555",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb35883",
   "metadata": {},
   "source": [
    "__DEFINING FUNCTIONS FOR SYLLABLE COUNT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7dd011",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()\n",
    "\n",
    "def nsyl(word):\n",
    "    count = 0\n",
    "    for x in d[word.lower()][0]:\n",
    "        if(x[-1].isdigit()):\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def syllables_count(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783413a3",
   "metadata": {},
   "source": [
    "__FORMING OUTPUT DATA STRUCTURE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efb14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "\n",
    "for idx in range(len(output_data)):\n",
    "    if(idx in [7,20,107]):\n",
    "        continue\n",
    "   \n",
    "    else:\n",
    "        with open(os.path.join('datatxt','{}.txt'.format(output_data['URL_ID'][idx])),'r', encoding = 'utf8') as file:\n",
    "            data = file.read()\n",
    "\n",
    "        Tokenizer = TreebankWordTokenizer()\n",
    "        tokenized_word = Tokenizer.tokenize(data)\n",
    "        \n",
    "        para = [p for p in data.split('\\n') if p]\n",
    "        x=[]\n",
    "        for p in para:\n",
    "            x.append(sent_tokenize(p))\n",
    "        tokenized_sent = [item for sublist in x for item in sublist]\n",
    "\n",
    "        english_stops = set(stopwords.words('english'))\n",
    "        words = [word for word in tokenized_word if word not in english_stops]\n",
    "\n",
    "        lm = WordNetLemmatizer()\n",
    "        punct = [i for i in string.punctuation]\n",
    "        final = [lm.lemmatize(word) for word in words if word not in punct]\n",
    "\n",
    "        with open('positive-words.txt','r') as f1:\n",
    "            list = f1.read()\n",
    "            positive_words = list.split('\\n')\n",
    "        with open('negative-words.txt','r') as f2:\n",
    "            list = f2.read()\n",
    "            negative_words = list.split('\\n')\n",
    "\n",
    "        syllable = []\n",
    "        for word in final:\n",
    "            if(word in d):\n",
    "                syllable.append(nsyl(word))\n",
    "            else:\n",
    "                syllable.append(syllables_count(word))\n",
    "\n",
    "        complex_words = [x for x in syllable if x>2]\n",
    "\n",
    "        char_word = [len(word) for word in final]  \n",
    "\n",
    "        pos = nltk.pos_tag(word_tokenize(data))\n",
    "        per_pronoun = [pos[i][0] for i in range(len(pos)) if pos[i][1] == 'PRP']\n",
    "\n",
    "        positive_score = len([word for word in final if word in positive_words])\n",
    "        negative_score = len([word for word in final if word in negative_words])\n",
    "        polarity_score = (positive_score-negative_score)/((positive_score + negative_score) + 0.000001)\n",
    "        subjectivity_score = (positive_score + negative_score)/ (len(final) + 0.000001)\n",
    "        avg_sent_len = len(final)/len(tokenized_sent)\n",
    "        complex_prcnt = round(100*(len(complex_words)/len(final)),5)\n",
    "        fog_index = 0.4*(avg_sent_len + complex_prcnt)\n",
    "        syl_per_wrd = sum(syllable)/len(final)\n",
    "        per_pro_count = len(per_pronoun)\n",
    "        avg_wrd_len = sum(char_word)/len(final)\n",
    "\n",
    "\n",
    "        output_data.iloc[idx,2:15] = [positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "                                   avg_sent_len, complex_prcnt, fog_index, avg_sent_len, len(complex_words),\n",
    "                                   len(final), syl_per_wrd, per_pro_count, avg_wrd_len]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438d2f2",
   "metadata": {},
   "source": [
    "__EDITING AND SAVING FILE IN XLSX FORMAT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d8620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
